// C&C Generals Zero Hour - Home Port
// Unicode String Support
//
// Original: UnicodeString.cpp (EA Games)
// Ported to Home with modern Unicode (UTF-8/UTF-16/UTF-32) support
//
// EA's original system supported UTF-16 for Windows wide strings.
// This port uses UTF-8 as the primary encoding (modern standard)
// with conversion utilities for UTF-16/UTF-32 when needed.
//
// Usage:
// ```
// let utf8_str = "Hello, ä¸–ç•Œ! ðŸŽ®"
// let utf16 = UnicodeString.to_utf16(utf8_str)
// let utf32 = UnicodeString.to_utf32(utf8_str)
//
// let char_count = UnicodeString.char_count(utf8_str)  // 12 (not byte count)
// ```

import basics/allocator
import basics/string

const MAX_UNICODE_STRING_LENGTH: u32 = 4096

// Unicode encoding types
enum UnicodeEncoding {
    UTF8
    UTF16LE  // Little-endian (Windows)
    UTF16BE  // Big-endian
    UTF32
}

// Unicode string operations
struct UnicodeString {
    // Convert UTF-8 to UTF-16 (for Windows APIs)
    fn to_utf16(utf8_str: string, allocator: Allocator): []u16 {
        let byte_len = utf8_str.len
        let mut utf16: []u16 = allocator.alloc_array<u16>(byte_len)  // Worst case
        let mut utf16_len: u32 = 0

        let mut i: u32 = 0
        while i < byte_len {
            let codepoint = decode_utf8_codepoint(utf8_str, &i)

            if codepoint <= 0xFFFF {
                // BMP (Basic Multilingual Plane) - single UTF-16 unit
                utf16[utf16_len] = @intCast(u16, codepoint)
                utf16_len += 1
            } else {
                // Supplementary plane - surrogate pair
                let adjusted = codepoint - 0x10000
                let high = 0xD800 + ((adjusted >> 10) & 0x3FF)
                let low = 0xDC00 + (adjusted & 0x3FF)

                utf16[utf16_len] = @intCast(u16, high)
                utf16[utf16_len + 1] = @intCast(u16, low)
                utf16_len += 2
            }
        }

        // Resize to actual length
        return utf16[0..utf16_len]
    }

    // Convert UTF-8 to UTF-32
    fn to_utf32(utf8_str: string, allocator: Allocator): []u32 {
        let byte_len = utf8_str.len
        let mut utf32: []u32 = allocator.alloc_array<u32>(byte_len)  // Worst case
        let mut utf32_len: u32 = 0

        let mut i: u32 = 0
        while i < byte_len {
            let codepoint = decode_utf8_codepoint(utf8_str, &i)
            utf32[utf32_len] = codepoint
            utf32_len += 1
        }

        return utf32[0..utf32_len]
    }

    // Convert UTF-16 to UTF-8
    fn from_utf16(utf16_str: []u16, allocator: Allocator): string {
        let utf16_len = utf16_str.len
        let mut utf8_bytes: []u8 = allocator.alloc_array<u8>(utf16_len * 4)  // Worst case
        let mut utf8_len: u32 = 0

        let mut i: u32 = 0
        while i < utf16_len {
            let codepoint = decode_utf16_codepoint(utf16_str, &i)
            utf8_len += encode_utf8_codepoint(codepoint, utf8_bytes, utf8_len)
        }

        return string.from_bytes(utf8_bytes[0..utf8_len])
    }

    // Get character count (not byte count)
    fn char_count(utf8_str: string): u32 {
        let byte_len = utf8_str.len
        let mut count: u32 = 0

        let mut i: u32 = 0
        while i < byte_len {
            decode_utf8_codepoint(utf8_str, &i)
            count += 1
        }

        return count
    }

    // Get character at index (0-based character index, not byte index)
    fn char_at(utf8_str: string, index: u32): u32 {
        let byte_len = utf8_str.len
        let mut char_index: u32 = 0

        let mut i: u32 = 0
        while i < byte_len {
            let codepoint = decode_utf8_codepoint(utf8_str, &i)

            if char_index == index {
                return codepoint
            }

            char_index += 1
        }

        return 0  // Invalid index
    }

    // Check if character is ASCII
    fn is_ascii(codepoint: u32): bool {
        return codepoint <= 0x7F
    }

    // Check if character is whitespace
    fn is_whitespace(codepoint: u32): bool {
        return codepoint == 0x20 or     // Space
               codepoint == 0x09 or     // Tab
               codepoint == 0x0A or     // LF
               codepoint == 0x0D or     // CR
               codepoint == 0xA0        // Non-breaking space
    }

    // Convert to uppercase (ASCII only for now)
    fn to_upper(utf8_str: string, allocator: Allocator): string {
        let bytes = utf8_str.as_bytes()
        let mut result: []u8 = allocator.alloc_array<u8>(bytes.len)

        for i in 0..bytes.len {
            let ch = bytes[i]
            if ch >= 'a' and ch <= 'z' {
                result[i] = ch - 32  // Convert to uppercase
            } else {
                result[i] = ch
            }
        }

        return string.from_bytes(result)
    }

    // Convert to lowercase (ASCII only for now)
    fn to_lower(utf8_str: string, allocator: Allocator): string {
        let bytes = utf8_str.as_bytes()
        let mut result: []u8 = allocator.alloc_array<u8>(bytes.len)

        for i in 0..bytes.len {
            let ch = bytes[i]
            if ch >= 'A' and ch <= 'Z' {
                result[i] = ch + 32  // Convert to lowercase
            } else {
                result[i] = ch
            }
        }

        return string.from_bytes(result)
    }
}

// Decode single UTF-8 codepoint and advance index
fn decode_utf8_codepoint(utf8_str: string, i: *u32): u32 {
    let bytes = utf8_str.as_bytes()
    let byte_len = bytes.len

    if i.* >= byte_len {
        return 0
    }

    let first_byte = bytes[i.*]
    i.* += 1

    // 1-byte sequence (ASCII)
    if (first_byte & 0x80) == 0 {
        return @intCast(u32, first_byte)
    }

    // 2-byte sequence
    if (first_byte & 0xE0) == 0xC0 {
        if i.* >= byte_len {
            return 0
        }

        let second_byte = bytes[i.*]
        i.* += 1

        let codepoint = ((@intCast(u32, first_byte) & 0x1F) << 6) |
                        (@intCast(u32, second_byte) & 0x3F)
        return codepoint
    }

    // 3-byte sequence
    if (first_byte & 0xF0) == 0xE0 {
        if i.* + 1 >= byte_len {
            return 0
        }

        let second_byte = bytes[i.*]
        let third_byte = bytes[i.* + 1]
        i.* += 2

        let codepoint = ((@intCast(u32, first_byte) & 0x0F) << 12) |
                        ((@intCast(u32, second_byte) & 0x3F) << 6) |
                        (@intCast(u32, third_byte) & 0x3F)
        return codepoint
    }

    // 4-byte sequence
    if (first_byte & 0xF8) == 0xF0 {
        if i.* + 2 >= byte_len {
            return 0
        }

        let second_byte = bytes[i.*]
        let third_byte = bytes[i.* + 1]
        let fourth_byte = bytes[i.* + 2]
        i.* += 3

        let codepoint = ((@intCast(u32, first_byte) & 0x07) << 18) |
                        ((@intCast(u32, second_byte) & 0x3F) << 12) |
                        ((@intCast(u32, third_byte) & 0x3F) << 6) |
                        (@intCast(u32, fourth_byte) & 0x3F)
        return codepoint
    }

    // Invalid UTF-8
    return 0
}

// Decode UTF-16 codepoint (handles surrogate pairs)
fn decode_utf16_codepoint(utf16_str: []u16, i: *u32): u32 {
    if i.* >= utf16_str.len {
        return 0
    }

    let first_unit = utf16_str[i.*]
    i.* += 1

    // BMP character (not surrogate)
    if first_unit < 0xD800 or first_unit > 0xDFFF {
        return @intCast(u32, first_unit)
    }

    // High surrogate - need low surrogate
    if first_unit >= 0xD800 and first_unit <= 0xDBFF {
        if i.* >= utf16_str.len {
            return 0  // Invalid - high surrogate without low
        }

        let second_unit = utf16_str[i.*]
        i.* += 1

        if second_unit < 0xDC00 or second_unit > 0xDFFF {
            return 0  // Invalid - not a low surrogate
        }

        let high = @intCast(u32, first_unit) - 0xD800
        let low = @intCast(u32, second_unit) - 0xDC00
        let codepoint = 0x10000 + (high << 10) + low

        return codepoint
    }

    // Invalid - low surrogate without high
    return 0
}

// Encode UTF-8 codepoint to bytes
fn encode_utf8_codepoint(codepoint: u32, buffer: []u8, offset: u32): u32 {
    // 1-byte sequence (ASCII)
    if codepoint <= 0x7F {
        buffer[offset] = @intCast(u8, codepoint)
        return 1
    }

    // 2-byte sequence
    if codepoint <= 0x7FF {
        buffer[offset] = @intCast(u8, 0xC0 | (codepoint >> 6))
        buffer[offset + 1] = @intCast(u8, 0x80 | (codepoint & 0x3F))
        return 2
    }

    // 3-byte sequence
    if codepoint <= 0xFFFF {
        buffer[offset] = @intCast(u8, 0xE0 | (codepoint >> 12))
        buffer[offset + 1] = @intCast(u8, 0x80 | ((codepoint >> 6) & 0x3F))
        buffer[offset + 2] = @intCast(u8, 0x80 | (codepoint & 0x3F))
        return 3
    }

    // 4-byte sequence
    if codepoint <= 0x10FFFF {
        buffer[offset] = @intCast(u8, 0xF0 | (codepoint >> 18))
        buffer[offset + 1] = @intCast(u8, 0x80 | ((codepoint >> 12) & 0x3F))
        buffer[offset + 2] = @intCast(u8, 0x80 | ((codepoint >> 6) & 0x3F))
        buffer[offset + 3] = @intCast(u8, 0x80 | (codepoint & 0x3F))
        return 4
    }

    // Invalid codepoint
    return 0
}

fn get_global_allocator(): Allocator {
    // TODO: Return global allocator
    return Allocator {}
}
